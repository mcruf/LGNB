} else if (RESPONSE == "AgeGroup" & INCLUDE == "both"){
datatot <- mybind(age_com, age_sur)
} else if (RESPONSE == "AgeGroup" & INCLUDE == "survey") {
datatot <- age_sur
} else if (RESPONSE == "AgeGroup" & INCLUDE == "commercial"){
datatot <- age_com
} else if (RESPONSE == "SizeGroup" & INCLUDE == "both"){
datatot <- mybind(size_com, size_sur)
} else if (RESPONSE == "SizeGroup" & INCLUDE == "survey"){
datatot <- size_sur
} else if(RESPONSE == "SizeGroup" & INCLUDE == "commercial")
datatot <- size_com
head(datatot)
# Load data file
#~~~~~~~~~~~~~~~~~~
comFULL <- readRDS("C:/Users/mruf/Desktop/LGCP_MSPTOOLS/LGNB_test/Data/obodfad_clean.rds"); colnames(comFULL)[5] <- "haulduration_hours" #Change name for convencience
comFULL$stock <- ifelse(comFULL$Area=="21","KAT","WBS")
comFULL$Data <- as.factor(paste("commercial"))
# Sample 15 hauls per year
#~~~~~~~~~~~~~~~~~~~~~~~~~~
com <- as.data.frame(comFULL %>%
group_by(Year) %>%
sample_n(size = 15))
table(com$Year) #Check if it worked
# Create a fake haul ID
#~~~~~~~~~~~~~~~~~~~~~~~~
com$HAULID <- as.factor(paste("c*_",1:nrow(com),sep=""))
# Create a fake vessel ID
#~~~~~~~~~~~~~~~~~~~~~~~~~~
levels(com$efid) <- paste("vessel_",1:nlevels(com$efid),sep="")
colnames(com)[12] <- "VESSELID"
# Shuffle the year-specific info
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
years <- 2005:2016
levels(com$Year) <- sample(years)
# Shuffle the metier-specific info
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
levels(com$metiers) <- sample(levels(com$metiers))
# Shuffle the age-specific info
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
com$age_1 <- sample(com$age_1)
com$age_2 <- sample(com$age_2)
com$age_3 <- sample(com$age_3)
com$age_4 <- sample(com$age_4)
com$age_5 <- sample(com$age_5)
com$age_6 <- sample(com$age_6)
tmp <- readRDS("C:/Users/mruf/OneDrive - Danmarks Tekniske Universitet/LNGB_tutorial_backup/old/Data/commercial.rds")
com$SG_1 <- sample(tmp$SG_1,nrow(com))
com$SG_2 <- sample(tmp$SG_2,nrow(com))
com$SG_3 <- sample(tmp$SG_3,nrow(com))
com$SG_4 <- sample(tmp$SG_4,nrow(com))
com$SG_5 <- sample(tmp$SG_5,nrow(com))
com$SG_6 <- sample(tmp$SG_6,nrow(com))
com$SG_7 <- sample(tmp$SG_7,nrow(com))
com$SG_8 <- sample(tmp$SG_8,nrow(com))
com$SG_9 <- sample(tmp$SG_9,nrow(com))
com$SG_10 <- sample(tmp$SG_10,nrow(com))
com$SG_11 <- sample(tmp$SG_11,nrow(com))
com$SG_12 <- sample(tmp$SG_12,nrow(com))
com$SG_13 <- sample(tmp$SG_13,nrow(com))
com$SG_14 <- sample(tmp$SG_14,nrow(com))
colnames(com)[29]
colnames(com)[29] <- "TimeYear"
colnames(com)
library(dplyr)
# Load data file
#~~~~~~~~~~~~~~~~~~
comFULL <- readRDS("C:/Users/mruf/Desktop/LGCP_MSPTOOLS/LGNB_test/Data/obodfad_clean.rds"); colnames(comFULL)[5] <- "haulduration_hours" #Change name for convencience
comFULL$stock <- ifelse(comFULL$Area=="21","KAT","WBS")
comFULL$Data <- as.factor(paste("commercial"))
# Sample 15 hauls per year
#~~~~~~~~~~~~~~~~~~~~~~~~~~
com <- as.data.frame(comFULL %>%
group_by(Year) %>%
sample_n(size = 15))
table(com$Year) #Check if it worked
# Create a fake haul ID
#~~~~~~~~~~~~~~~~~~~~~~~~
com$HAULID <- as.factor(paste("c*_",1:nrow(com),sep=""))
# Create a fake vessel ID
#~~~~~~~~~~~~~~~~~~~~~~~~~~
levels(com$efid) <- paste("vessel_",1:nlevels(com$efid),sep="")
colnames(com)[12] <- "VESSELID"
head(com)
# Shuffle the year-specific info
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
years <- 2005:2016
levels(com$Year) <- sample(years)
# Shuffle the metier-specific info
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
levels(com$metiers) <- sample(levels(com$metiers))
# Shuffle the age-specific info
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
com$age_1 <- sample(com$age_1)
com$age_2 <- sample(com$age_2)
com$age_3 <- sample(com$age_3)
com$age_4 <- sample(com$age_4)
com$age_5 <- sample(com$age_5)
com$age_6 <- sample(com$age_6)
head(com)
tmp <- readRDS("C:/Users/mruf/OneDrive - Danmarks Tekniske Universitet/LNGB_tutorial_backup/old/Data/commercial.rds")
com$SG_1 <- sample(tmp$SG_1,nrow(com))
com$SG_2 <- sample(tmp$SG_2,nrow(com))
com$SG_3 <- sample(tmp$SG_3,nrow(com))
com$SG_4 <- sample(tmp$SG_4,nrow(com))
com$SG_5 <- sample(tmp$SG_5,nrow(com))
com$SG_6 <- sample(tmp$SG_6,nrow(com))
com$SG_7 <- sample(tmp$SG_7,nrow(com))
com$SG_8 <- sample(tmp$SG_8,nrow(com))
com$SG_9 <- sample(tmp$SG_9,nrow(com))
com$SG_10 <- sample(tmp$SG_10,nrow(com))
com$SG_11 <- sample(tmp$SG_11,nrow(com))
com$SG_12 <- sample(tmp$SG_12,nrow(com))
com$SG_13 <- sample(tmp$SG_13,nrow(com))
com$SG_14 <- sample(tmp$SG_14,nrow(com))
colnames(com)[29] <- "TimeYear"
colnames(com)
#########################################
#                                       #
#   Prepare data for github tutorial    #
#                                       #
#########################################
library(dplyr)
# Load data file
#~~~~~~~~~~~~~~~~~~
comFULL <- readRDS("C:/Users/mruf/Desktop/LGCP_MSPTOOLS/LGNB_test/Data/obodfad_clean.rds"); colnames(comFULL)[5] <- "haulduration_hours" #Change name for convencience
comFULL$stock <- ifelse(comFULL$Area=="21","KAT","WBS")
comFULL$Data <- as.factor(paste("commercial"))
# Sample 15 hauls per year
#~~~~~~~~~~~~~~~~~~~~~~~~~~
com <- as.data.frame(comFULL %>%
group_by(Year) %>%
sample_n(size = 15))
table(com$Year) #Check if it worked
# Create a fake haul ID
#~~~~~~~~~~~~~~~~~~~~~~~~
com$HAULID <- as.factor(paste("c*_",1:nrow(com),sep=""))
# Create a fake vessel ID
#~~~~~~~~~~~~~~~~~~~~~~~~~~
levels(com$efid) <- paste("vessel_",1:nlevels(com$efid),sep="")
colnames(com)[12] <- "VESSELID"
# Shuffle the year-specific info
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
years <- 2005:2016
levels(com$Year) <- sample(years)
# Shuffle the metier-specific info
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
levels(com$metiers) <- sample(levels(com$metiers))
# Shuffle the age-specific info
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
com$age_1 <- sample(com$age_1)
com$age_2 <- sample(com$age_2)
com$age_3 <- sample(com$age_3)
com$age_4 <- sample(com$age_4)
com$age_5 <- sample(com$age_5)
com$age_6 <- sample(com$age_6)
head(com)
tmp <- readRDS("C:/Users/mruf/OneDrive - Danmarks Tekniske Universitet/LNGB_tutorial_backup/old/Data/commercial.rds")
com$SG_1 <- sample(tmp$SG_1,nrow(com))
com$SG_2 <- sample(tmp$SG_2,nrow(com))
com$SG_3 <- sample(tmp$SG_3,nrow(com))
com$SG_4 <- sample(tmp$SG_4,nrow(com))
com$SG_5 <- sample(tmp$SG_5,nrow(com))
com$SG_6 <- sample(tmp$SG_6,nrow(com))
com$SG_7 <- sample(tmp$SG_7,nrow(com))
com$SG_8 <- sample(tmp$SG_8,nrow(com))
com$SG_9 <- sample(tmp$SG_9,nrow(com))
com$SG_10 <- sample(tmp$SG_10,nrow(com))
com$SG_11 <- sample(tmp$SG_11,nrow(com))
com$SG_12 <- sample(tmp$SG_12,nrow(com))
com$SG_13 <- sample(tmp$SG_13,nrow(com))
com$SG_14 <- sample(tmp$SG_14,nrow(com))
head(com)
colnames(com)[29] <- "TimeYear"
colnames(com)
colnames(com)[23:27]
colnames(com)[c(23:27,35)]
colnames(com)[c(23:27,35)] <- paste("Age",1:6,sep="")
# Keep only the most important columns
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
df <- com[,c("HAULID","VESSELID","haulduration_hours","latStart","lonStart","latEnd","lonEnd","Area",
"metiers","Month","Quarter","Year","TimeYear","stock","Data",paste("Age",1:6,sep="_"),
paste("SG",1:14,sep="_"))]
colnames(com)[c(23:27,35)]
colnames(com)[c(23:27,35)] <- paste("Age",1:6,sep="_")
colnames(com)
#########################################
#                                       #
#   Prepare data for github tutorial    #
#                                       #
#########################################
library(dplyr)
# Load data file
#~~~~~~~~~~~~~~~~~~
comFULL <- readRDS("C:/Users/mruf/Desktop/LGCP_MSPTOOLS/LGNB_test/Data/obodfad_clean.rds"); colnames(comFULL)[5] <- "haulduration_hours" #Change name for convencience
comFULL$stock <- ifelse(comFULL$Area=="21","KAT","WBS")
comFULL$Data <- as.factor(paste("commercial"))
# Sample 15 hauls per year
#~~~~~~~~~~~~~~~~~~~~~~~~~~
com <- as.data.frame(comFULL %>%
group_by(Year) %>%
sample_n(size = 15))
table(com$Year) #Check if it worked
# Create a fake haul ID
#~~~~~~~~~~~~~~~~~~~~~~~~
com$HAULID <- as.factor(paste("c*_",1:nrow(com),sep=""))
# Create a fake vessel ID
#~~~~~~~~~~~~~~~~~~~~~~~~~~
levels(com$efid) <- paste("vessel_",1:nlevels(com$efid),sep="")
colnames(com)[12] <- "VESSELID"
# Shuffle the year-specific info
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
years <- 2005:2016
levels(com$Year) <- sample(years)
# Shuffle the metier-specific info
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
levels(com$metiers) <- sample(levels(com$metiers))
# Shuffle the age-specific info
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
com$age_1 <- sample(com$age_1)
com$age_2 <- sample(com$age_2)
com$age_3 <- sample(com$age_3)
com$age_4 <- sample(com$age_4)
com$age_5 <- sample(com$age_5)
com$age_6 <- sample(com$age_6)
# Include fake N_size columns
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# borrow info from the old dataset
tmp <- readRDS("C:/Users/mruf/OneDrive - Danmarks Tekniske Universitet/LNGB_tutorial_backup/old/Data/commercial.rds")
com$SG_1 <- sample(tmp$SG_1,nrow(com))
com$SG_2 <- sample(tmp$SG_2,nrow(com))
com$SG_3 <- sample(tmp$SG_3,nrow(com))
com$SG_4 <- sample(tmp$SG_4,nrow(com))
com$SG_5 <- sample(tmp$SG_5,nrow(com))
com$SG_6 <- sample(tmp$SG_6,nrow(com))
com$SG_7 <- sample(tmp$SG_7,nrow(com))
com$SG_8 <- sample(tmp$SG_8,nrow(com))
com$SG_9 <- sample(tmp$SG_9,nrow(com))
com$SG_10 <- sample(tmp$SG_10,nrow(com))
com$SG_11 <- sample(tmp$SG_11,nrow(com))
com$SG_12 <- sample(tmp$SG_12,nrow(com))
com$SG_13 <- sample(tmp$SG_13,nrow(com))
com$SG_14 <- sample(tmp$SG_14,nrow(com))
# Change some colum names
#~~~~~~~~~~~~~~~~~~~~~~~~~~
colnames(com)[29] <- "TimeYear"
colnames(com)[c(23:27,35)] <- paste("Age",1:6,sep="_")
# Keep only the most important columns
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
df <- com[,c("HAULID","VESSELID","haulduration_hours","latStart","lonStart","latEnd","lonEnd","Area",
"metiers","Month","Quarter","Year","TimeYear","stock","Data",paste("Age",1:6,sep="_"),
paste("SG",1:14,sep="_"))]
saveRDS(df, "C:/Users/mruf/Documents/LGNB/Data/commercial.rds")
##########################################################################################
#                                                                                        #
##              LGNB model: A practical statistical framework to combine                ##
##               commercial & survey data to model the spatio-temporal                  ##
##                          dynamics of marine harvested species                        ##
##                                    (Rufener et al.)                                  ##
#                                                                                        #
##########################################################################################
# last update: April 2020
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Section 1: Default inputs
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
STOCK <- c("WBS","KAT")[1] # Specify to which stock the data should be subsetted (Western Baltic Sea or Kattegat)
INCLUDE  <- c("commercial", "survey", "both") [3] # Specify the desired input data for the model; default is commercial data-
RESPONSE <- c("SizeGroup","AgeGroup","Cohort")[2] #Choose whether the model is applied for each SizeGroup, AgeGroup, or on a Cohort basis (when Nage).Default is set to SizeGroup.
ALPHA <- c("No","Single")[1] # Define how many alpha parameters should be computed
# @ ALPHA = "No" -> Models without alpha parameter; can be applied either to model with one or several support areas.
# @ ALPHA = "Single" -> Models with one alpha parameter for each data soruce; Used when commercial data are described by only one support area for the entire time series; Also applied when using survey data;
# We go from the assumption that survey data has ALWAYS only one support area;
# one can choose whether to estimate the alpha-parameter or not.
if(INCLUDE == "survey"){
SUPPORT_AREA <- "One"
ALPHA <- c("No","Single")[1]
}
# Specify the model structure; default model is m2 (see lines ... to ...) for either size groups, age groups or cohort
# Default size group is 5 (S5), age group is 3 (A3) and cohort from 2005.
if(RESPONSE == "SizeGroup"){
MODEL_CONFIG <- "m2_S5" #Default model and SizeGroup 5
}else if(RESPONSE == "AgeGroup"){
MODEL_CONFIG <- "m2_A3" #Default model and AgeGroup 3
} else if(RESPONSE == "Cohort")
MODEL_CONFIG <- "m1_2015" #Default model when applied on cohort-basis (2005 cohort)
MODEL_CONFIG <- strsplit(MODEL_CONFIG, "_")[[1]]
MODEL_FORMULA <- MODEL_CONFIG[1]
if(RESPONSE == "SizeGroup"){
SIZE <- MODEL_CONFIG[2]
} else if(RESPONSE == "AgeGroup"){
AGE <- MODEL_CONFIG[2]
} else if(RESPONSE == "Cohort")
YEARCLASS <- MODEL_CONFIG[2]
# For scripting (Useful when running on a HPC)
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
input <- parse(text=Sys.getenv("SCRIPT_INPUT"))
print(input)
eval(input)
stopifnot(INCLUDE %in% c("commercial", "survey", "both"))
#><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Section 2: Load data files & R packages
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# 2.1) Load helper functions and R libraries
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
source("C:/Users/mruf/Documents/LGNB/R/utilities.R")
#devtools::install_github("kaskr/gridConstruct",subdir="gridConstruct") # To install the gridConstruct package
mLoad(raster,rgeos,maptools,maps,data.table,dplyr,TMB,sp,
DATRAS,gridConstruct,rgdal,geosphere,devtools,plyr,fields)
# 2.2.1) Load commercial fisheries data (fishery-depedent)
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
comFULL <- readRDS("C:/Users/mruf/Documents/LGNB/Data/commercial.rds")
comFULL$stock <- ifelse(comFULL$Area=="21","KAT","WBS")
# Subset data for a particular stock
commercial <- subset(comFULL, stock == STOCK) #Setting stock based on ICES area (KAT=21, WBS=22-24)
# Drop unused factor levels
commercial[,c("Month","Year","Quarter","Area","metiers","Data","HAULID","VESSELID","TimeYear")] <- lapply(commercial[,c("Month","Year","Quarter","Area","metiers","Data","HAULID","VESSELID","TimeYear")], factor)
# 2.2.2) Load scientific survey data (Fishery~indepdendent)
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
survey <- readRDS("C:/Users/mruf/Documents/LGNB/Data/survey.rds")
survey$stock <- ifelse(survey$Area=="21","KAT","WBS") #Setting stock based on ICES area (KAT=21, WBS=22-24)
# Subset data for a particular time frame
survey <- filter(survey, stock == STOCK)
# Drop unused factor levels
survey[,c("Haul_ID","Month","Year","Quarter","Area","Data","TimeYear","Sediment")] <- lapply(survey[,c("Haul_ID","Month","Year","Quarter","Area","Data","TimeYear","Sediment")], factor)
#><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Section 3: Binding survey and commercial data files
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Binding depends whether response variable is on a cohort basis or AgeGroup/SizeGroup basis
# 3.1.1) Cohort-basis
#~~~~~~~~~~~~~~~~~~~~~~~
# First we need to create a dataframe in such way that both datasets have the same timelevels;
# This is VERY important, as uneven timelevels will cause problems for the AR1 process.
if(RESPONSE == "Cohort"){
df_cohort  <- extractCohortLevels(commercial,survey,yearclass = as.numeric(YEARCLASS)) #Df with equal time-steps
NageGroup  <- length(grep("Age_", names(commercial), value = TRUE))
tim        <- (as.numeric(YEARCLASS) + NageGroup)-1 #
df_cohort  <- subset(df_cohort, Year %in% c(YEARCLASS:tim))
cohort_com <- extract_cohort_quarter(commercial,df_cohort) #Extract cohort for commercial data
cohort_sur <- extract_cohort_quarter(survey,df_cohort) #Extract cohort for survey data
cohort_com <- transform(cohort_com, HLID=HAULID, HaulDur=haulduration_hours)
cohort_sur <- transform(cohort_sur, latStart=lat, lonStart=lon, latEnd=lat, lonEnd=lon, HLID=Haul_ID)
# 3.1.2) AgeGroup-basis
#~~~~~~~~~~~~~~~~~~~~~~~~
} else if (RESPONSE == "AgeGroup"){
age_com   <- transform(commercial, HLID=HAULID, HaulDur=haulduration_hours)
age_sur   <- transform(survey, latStart=lat, lonStart=lon, latEnd=lat, lonEnd=lon,HLID=Haul_ID)
# 3.1.3) SizeGroup-basis
#~~~~~~~~~~~~~~~~~~~~~~~~
} else if(RESPONSE == "SizeGroup"){
size_com <- transform(commercial, HLID=HAULID, HaulDur=haulduration_hours)
size_sur <- transform(survey, latStart=lat, lonStart=lon, latEnd=lat, lonEnd=lon,HLID=Haul_ID)
}
# 3.2) Bind both datasets into a single data frame
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
if(RESPONSE == "Cohort" & INCLUDE =="both"){
datatot <- mybind(cohort_com, cohort_sur)
} else if(RESPONSE == "Cohort" & INCLUDE == "survey"){
datatot <- cohort_sur
} else if(RESPONSE == "Cohort" & INCLUDE == "commercial"){
datatot <- cohort_com
} else if (RESPONSE == "AgeGroup" & INCLUDE == "both"){
datatot <- mybind(age_com, age_sur)
} else if (RESPONSE == "AgeGroup" & INCLUDE == "survey") {
datatot <- age_sur
} else if (RESPONSE == "AgeGroup" & INCLUDE == "commercial"){
datatot <- age_com
} else if (RESPONSE == "SizeGroup" & INCLUDE == "both"){
datatot <- mybind(size_com, size_sur)
} else if (RESPONSE == "SizeGroup" & INCLUDE == "survey"){
datatot <- size_sur
} else if(RESPONSE == "SizeGroup" & INCLUDE == "commercial")
datatot <- size_com
head(datatot)
# 3.3.1) For AgeGroups
#~~~~~~~~~~~~~~~~~~~~~~
if(RESPONSE == "AgeGroup"){
if(AGE == "A0"){
datatot$Response <- as.numeric(paste(datatot$Age_0))
} else if(AGE == "A1"){
datatot$Response <- as.numeric(paste(datatot$Age_1))
} else if (AGE == "A2") {
datatot$Response <- as.numeric(paste(datatot$Age_2))
} else if (AGE == "A3"){
datatot$Response <- as.numeric(paste(datatot$Age_3))
} else if (AGE == "A4"){
datatot$Response <- as.numeric(paste(datatot$Age_4))
} else if (AGE == "A5"){
datatot$Response <- as.numeric(paste(datatot$Age_5))
} else if (AGE == "A6"){
datatot$Response <- as.numeric(paste(datatot$Age_6))
} else if (AGE == "A7"){
datatot$Response <- as.numeric(paste(datatot$Age_7))
} else if (AGE == "A8"){
datatot$Response <- as.numeric(paste(datatot$Age_8))
}
}
# 3.3.2) For SizeGroups
#~~~~~~~~~~~~~~~~~~~~~~~
if(RESPONSE == "SizeGroup"){
if(SIZE == "S1"){
datatot$Response <- as.numeric(paste(datatot$SG_1))
} else if (SIZE == "S2") {
datatot$Response <- as.numeric(paste(datatot$SG_2))
} else if (SIZE == "S3"){
datatot$Response <- as.numeric(paste(datatot$SG_3))
} else if (SIZE == "S4"){
datatot$Response <- as.numeric(paste(datatot$SG_4))
} else if (SIZE == "S5"){
datatot$Response <- as.numeric(paste(datatot$SG_5))
} else if (SIZE == "S6"){
datatot$Response <- as.numeric(paste(datatot$SG_6))
} else if (SIZE == "S7"){
datatot$Response <- as.numeric(paste(datatot$SG_7))
} else if (SIZE == "S8"){
datatot$Response <- as.numeric(paste(datatot$SG_8))
} else if (SIZE == "S9"){
datatot$Response <- as.numeric(paste(datatot$SG_9))
} else if (SIZE == "S10"){
datatot$Response <- as.numeric(paste(datatot$SG_10))
} else if (SIZE == "S11"){
datatot$Response <- as.numeric(paste(datatot$SG_11))
} else if (SIZE == "S12"){
datatot$Response <- as.numeric(paste(datatot$SG_12))
} else if (SIZE == "S13"){
datatot$Response <- as.numeric(paste(datatot$SG_13))
} else if (SIZE =="S14"){
datatot$Response <- as.numeric(paste(datatot$SG_14))
}
}
# 3.4) Create equally time spaced intervals - VERY important for the AR1 process
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
datatot$Year <- as.numeric(as.character(datatot$Year))
timeLevels <- as.vector(t(outer(min(datatot$Year):max(datatot$Year), 1:4, paste)))
datatot$YearQuarter <- factor(paste(datatot$Year, datatot$Quarter), levels=timeLevels)
# 4.1) Creating a dataframe with mean values of long and lat
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
comFULL$lon_mean <- rowMeans(comFULL[,c("lonStart", "lonEnd")])
comFULL$lat_mean <- rowMeans(comFULL[,c("latStart", "latEnd")])
df <- data.frame(lon=comFULL$lon_mean, lat=comFULL$lat_mean) #temporary df
# 4.2) Building the grid
#~~~~~~~~~~~~~~~~~~~~~~~~~
if(.Platform$OS.type == "windows") setwd("C:/Users/mruf/Documents/LGNB/Shapefiles")
grid <- gridConstruct2(df,km=10,scale=1.2)
gr <- gridFilter2(grid,df,icesSquare = T,connected=T) # filter out unnecessary spatial extensions
plot(gr)
# 5.1) Setting a data frame containing the haul ID, and start and end long/lat of the haul
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
dat <- data.frame(sampleID=datatot$HLID, start_long=datatot$lonStart,
start_lat=datatot$latStart, end_long=datatot$lonEnd, end_lat=datatot$latEnd)
# 5.2) Define a matrix for the haulÂ´s start and end position
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
mStart <- matrix(c(dat$start_long,dat$start_lat), ncol=2)
mEnd <- matrix(c(dat$end_long,dat$end_lat), ncol=2)
# 5.3) Interpolate points at regular distance (default is 1km)
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
nbpts <- floor(distance(dat$start_long,dat$start_lat, dat$end_long, dat$end_lat) / 1) # one point every 1 km ~ reasonable for a 5x5 km grid
inter_pts <- gcIntermediate(mStart,mEnd, n=nbpts, addStartEnd=FALSE)
# 5.4) Associate the discretized hauls to the grid ID
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Note that the haul Id MUST be a factor, where each level is the frequency of a particular haul crossing a specific grid ID
tmp <- lapply(1:length(inter_pts), function(i) {
print(i)
x <- inter_pts[[i]]
colnames(x) <- c("lon", "lat") #Needs to be the same names as those in inter_pts
x <- as.data.frame(x)
haul.id <- datatot$HLID[i] #Pick the specific haul id
ind <- gridLocate(gr, x) #Locate the grid ID
data.frame(haul.id=haul.id, ind=ind, rowID = i)
})
tmp2 <- do.call("rbind", tmp)
tmp2$haulid <- factor(tmp2$haul.id)
tmp2$gf <- factor(tmp2$ind, 1:nrow(gr))
tmp2 <- tmp2[c("haulid","gf","rowID")]
datatot$split_area <- ifelse(as.character(datatot$Data)=="commercial", "commercial", "survey") #Takes the haul positions of the aggregated time-series
datatot$split_area <- as.factor(datatot$split_area) #IMPORTANT - needs to be a factor!!
tmpOne <- tmp2; tmpOne$split <- datatot$split_area[tmp2$rowID]
SupportAreaMatrix    <- table(tmpOne$gf, tmpOne$split)
SupportAreaMatrix[]  <- SupportAreaMatrix>0
SupportAreaMatrix    <- ifelse(SupportAreaMatrix==0,FALSE,TRUE)
if(INCLUDE == "commercial" || "both"){
SupportAreaMatrix <- SupportAreaMatrix}
INLCUDE
INCLUDE
if(INCLUDE == "commercial" || INCLUDE=="both"){
SupportAreaMatrix <- SupportAreaMatrix}
image(gr, SupportAreaMatrix[,1]) #To see the progress..
#
# 6.3) Map haulid to data.frame rows
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# VERY IMPORTANT: haulid must match with the dataframe's row number (in increasing order)
rowid <- match(as.character(tmp2$haulid),as.character(datatot$HLID))
stopifnot(all(is.finite(rowid)))
rowid <- factor(rowid)
# 7.1) Sparse matrices for GMRF: Q = Q0+delta*I
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Q0 <- -attr(gr,"pattern")
diag(Q0) <- 0
diag(Q0) <- -rowSums(Q0)
I <- .symDiagonal(nrow(Q0))
# 7.2) Compile LGNB model
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~
if(.Platform$OS.type == "windows") setwd("C:/Users/mruf/Documents/LGNB/src") #Set your own directory where the C++ is stored
Sys.setenv(PATH="%PATH%;C:/Rtools/gcc-4.6.3/bin;c:/Rtools/bin") #Run only when on windows
compile("LGNB.cpp")
dyn.load(dynlib("LGNB"))
# 7.2) Compile LGNB model
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~
if(.Platform$OS.type == "windows") setwd("C:/Users/mruf/Documents/LGNB/src") #Set your own directory where the C++ is stored
#Sys.setenv(PATH="%PATH%;C:/Rtools/gcc-4.6.3/bin;c:/Rtools/bin") #Run only when on windows
compile("LGNB.cpp")
